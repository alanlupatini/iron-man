{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62c67f1-72e7-4fa3-9cf8-29acc10e7e7f",
   "metadata": {},
   "source": [
    "## Initial Merge of raw databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e1d65-71ca-4c58-a729-15c39872abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = r'..\\data\\raw'\n",
    "file1 = os.path.join(folder_path, 'or1.xlsx')\n",
    "file2 = os.path.join(folder_path, 'or2.xlsx')\n",
    "\n",
    "df1 = pd.read_excel(file1)\n",
    "df2 = pd.read_excel(file2)\n",
    "\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "output_path_csv = os.path.join(folder_path, 'merged_ironman_data.csv')\n",
    "merged_df.to_csv(output_path_csv, index=False)\n",
    "\n",
    "print(f\"Success! Saved as CSV: {output_path_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd77d4-06b2-427b-b3a7-39be8dc2897b",
   "metadata": {},
   "source": [
    "## First data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d2211-f9e2-4b5a-8e18-feb71efab2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column Names:\", merged_df.columns.tolist())\n",
    "\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(merged_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27d4b8-0c3b-4390-bdbe-0e80b66c7d78",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e16121-77da-4499-9161-265ed066571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['InvoiceNo'] = merged_df['InvoiceNo'].fillna(merged_df['Invoice'])\n",
    "merged_df['UnitPrice'] = merged_df['UnitPrice'].fillna(merged_df['Price'])\n",
    "merged_df['CustomerID'] = merged_df['CustomerID'].fillna(merged_df['Customer ID'])\n",
    "\n",
    "cols_to_drop = ['Invoice', 'Price', 'Customer ID']\n",
    "merged_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# Handle Missing Values\n",
    "merged_df['Description'] = merged_df['Description'].fillna('Unknown')\n",
    "\n",
    "# CustomerID\n",
    "merged_df['CustomerID'] = merged_df['CustomerID'].fillna(0).astype(int)\n",
    "\n",
    "# Data Type Correction\n",
    "merged_df['InvoiceDate'] = pd.to_datetime(merged_df['InvoiceDate'])\n",
    "\n",
    "print(\"Cleaned Data Overview:\")\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bf533-104b-420e-9543-3ecf14ebb1e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d49f11-42b6-4597-ada2-5fe2fa876a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7ef557-27ca-4e1a-9425-37e2c1b60c58",
   "metadata": {},
   "source": [
    "## Features ready for ML model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651727b4-3056-4ad9-b44c-c61be3cfa92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Customers: {merged_df['CustomerID'].nunique()}\")\n",
    "print(f\"Unique Products: {merged_df['StockCode'].nunique()}\")\n",
    "\n",
    "import numpy as np\n",
    "print(\"\\nAny infinite values?\")\n",
    "print(np.isinf(merged_df.select_dtypes(include=np.number)).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d7603-3733-4fa0-904d-6e4f593b8bb3",
   "metadata": {},
   "source": [
    "## Saving cleaned DF for Project Iron Man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2628b-4083-466f-b98d-f5e78ce99c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Reload the raw merged data\n",
    "\n",
    "# 2. Safe Merge and Type Conversion (Same as before)\n",
    "if 'Invoice' in merged_df.columns:\n",
    "    merged_df['InvoiceNo'] = merged_df['InvoiceNo'].fillna(merged_df['Invoice'])\n",
    "if 'Price' in merged_df.columns:\n",
    "    merged_df['UnitPrice'] = merged_df['UnitPrice'].fillna(merged_df['Price'])\n",
    "if 'Customer ID' in merged_df.columns:\n",
    "    merged_df['CustomerID'] = merged_df['CustomerID'].fillna(merged_df['Customer ID'])\n",
    "\n",
    "merged_df['InvoiceNo'] = merged_df['InvoiceNo'].astype(str)\n",
    "merged_df['CustomerID'] = merged_df['CustomerID'].fillna(0)\n",
    "\n",
    "# 3. THE CHANGE: Smart Filtering\n",
    "cleaned_df = merged_df[merged_df['UnitPrice'] > 0].copy()\n",
    "\n",
    "# 4. Create an 'Is_Return' flag for easier ML processing later\n",
    "cleaned_df['Is_Return'] = cleaned_df['Quantity'] < 0\n",
    "\n",
    "print(\"New Cleaning Results:\")\n",
    "print(f\"Total Transactions: {len(cleaned_df)}\")\n",
    "print(f\"Number of Returns Preserved: {cleaned_df['Is_Return'].sum()}\")\n",
    "\n",
    "# 5. Overwrite your clean parquet file\n",
    "output_path = r'C:\\Users\\ASUS\\Desktop\\courses\\projects\\iron-man\\data\\clean\\ironman.parquet'\n",
    "cleaned_df.to_parquet(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f13a24-26e3-4687-9721-e45b5398a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Redoing data cleaning to not remove neg sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323d310-2d36-4b10-a6e1-67cbb2a630d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r'C:\\Users\\ASUS\\Desktop\\courses\\projects\\iron-man\\data\\raw'\n",
    "\n",
    "if 'Invoice' in merged_df.columns:\n",
    "    merged_df['InvoiceNo'] = merged_df['InvoiceNo'].fillna(merged_df['Invoice'])\n",
    "\n",
    "if 'Price' in merged_df.columns:\n",
    "    merged_df['UnitPrice'] = merged_df['UnitPrice'].fillna(merged_df['Price'])\n",
    "\n",
    "if 'Customer ID' in merged_df.columns:\n",
    "    merged_df['CustomerID'] = merged_df['CustomerID'].fillna(merged_df['Customer ID'])\n",
    "\n",
    "cols_to_drop = ['Invoice', 'Price', 'Customer ID']\n",
    "existing_cols_to_drop = [c for c in cols_to_drop if c in merged_df.columns]\n",
    "merged_df.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "\n",
    "merged_df['InvoiceNo'] = merged_df['InvoiceNo'].astype(str)\n",
    "merged_df['StockCode'] = merged_df['StockCode'].astype(str)\n",
    "merged_df['InvoiceDate'] = pd.to_datetime(merged_df['InvoiceDate'])\n",
    "\n",
    "merged_df = merged_df[(merged_df['Quantity'] > 0) & (merged_df['UnitPrice'] > 0)]\n",
    "\n",
    "output_parquet = os.path.join(folder_path, 'ironman2.parquet')\n",
    "merged_df.to_parquet(output_parquet, index=False)\n",
    "\n",
    "print(f\"File cleaned and saved successfully to: {output_parquet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d66ec-c8fc-40f8-9dcb-51264e335ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cdf6a-b2fc-4c14-b48f-02f70e0e3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# LOAD & FIX THE DATA\n",
    "raw_file = r'C:\\Users\\ASUS\\Desktop\\courses\\projects\\iron-man\\data\\raw\\merged_ironman_data.csv'\n",
    "\n",
    "print(f\"Reading file: {raw_file}...\")\n",
    "# encoding='ISO-8859-1' helps if the CSV has weird currency symbols like Â£\n",
    "df = pd.read_csv(raw_file, encoding='ISO-8859-1')\n",
    "\n",
    "# fill the ghosts (missing IDs)\n",
    "df['CustomerID'] = df['CustomerID'].fillna(0)\n",
    "\n",
    "# filter out items with 0 price (junk), BUT we keep negative Quantities (returns)\n",
    "df = df[df['UnitPrice'] > 0]\n",
    "\n",
    "# Calculate TotalPrice\n",
    "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "# Save this version so we stop losing our returns!\n",
    "clean_path = r'C:\\Users\\ASUS\\Desktop\\courses\\projects\\iron-man\\data\\clean\\ironman2.parquet'\n",
    "df.to_parquet(clean_path, index=False)\n",
    "print(f\"Saved fixed file to: {clean_path}\")\n",
    "\n",
    "# Check if we actually have returns now\n",
    "num_returns = len(df[df['Quantity'] < 0])\n",
    "print(f\"SUCCESS! Found {num_returns} return rows.\")\n",
    "\n",
    "\n",
    "# --- PART 2: THE GRAPH THAT WAS BLANK ---\n",
    "if num_returns > 0:\n",
    "    # 1. Clean descriptions\n",
    "    df['Description'] = df['Description'].astype(str).str.strip()\n",
    "\n",
    "    # 2. Get Sales (Sold)\n",
    "    sales = df[df['Quantity'] > 0].groupby('Description')['Quantity'].sum().reset_index()\n",
    "    sales.columns = ['Description', 'Sold']\n",
    "\n",
    "    # 3. Get Returns (Returned) - fix negatives first\n",
    "    rets = df[df['Quantity'] < 0].copy()\n",
    "    rets['Qty_Abs'] = rets['Quantity'].abs()\n",
    "    returns = rets.groupby('Description')['Qty_Abs'].sum().reset_index()\n",
    "    returns.columns = ['Description', 'Returned']\n",
    "\n",
    "    # 4. Merge\n",
    "    stats = pd.merge(sales, returns, on='Description', how='inner')\n",
    "    \n",
    "    # 5. Math time\n",
    "    stats['Return_Prob'] = (stats['Returned'] / stats['Sold']) * 100\n",
    "    \n",
    "    # 6. Filter: Only show items sold > 50 times (to ignore rare stuff)\n",
    "    top_junk = stats[stats['Sold'] > 50].sort_values('Return_Prob', ascending=False).head(10)\n",
    "\n",
    "    # 7. Plot\n",
    "    print(\"\\nGenerating the Headaches Graph...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=top_junk, x='Return_Prob', y='Description', palette='Reds_r')\n",
    "    plt.title('Top 10 Products by Return Probability (Items sold > 50 times)')\n",
    "    plt.xlabel('Return Probability (%)')\n",
    "    plt.show()\n",
    "\n",
    "    # 8. Print the list\n",
    "    print(\"\\nTop risky items:\")\n",
    "    print(top_junk[['Description', 'Sold', 'Returned', 'Return_Prob']])\n",
    "\n",
    "else:\n",
    "    print(\"0 returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb93a03-c432-40f4-b67c-d87ec504149c",
   "metadata": {},
   "source": [
    "## Saving it into a SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e063868-d598-4ce2-9f0a-2b40b38201fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to ironman...\n",
      "Uploading... this may take a moment.\n",
      "Success! Data is now in MySQL.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Load credentials\n",
    "load_dotenv()\n",
    "user = os.getenv('DB_USER')\n",
    "password = os.getenv('DB_PASSWORD')\n",
    "host = os.getenv('DB_HOST')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "# 2. Load Parquet\n",
    "file_path = r\"C:\\Users\\ASUS\\Desktop\\courses\\projects\\iron-man\\data\\clean\\ironman2.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "# 3. Direct Connection (Assuming 'ironman' database already exists)\n",
    "# We use a single engine to keep things stable\n",
    "engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}/{db_name}')\n",
    "\n",
    "print(f\"Connecting to {db_name}...\")\n",
    "\n",
    "try:\n",
    "    # 4. Upload Data\n",
    "    # chunksize helps if the file is large, preventing memory overflows\n",
    "    print(\"Uploading... this may take a moment.\")\n",
    "    df.to_sql('transactions', con=engine, if_exists='replace', index=False, chunksize=1000)\n",
    "    print(\"Success! Data is now in MySQL.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Always close the engine to prevent future locks\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da783770-d9ec-4ac8-ab56-aadd6d02b6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
